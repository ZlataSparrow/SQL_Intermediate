{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import lit\n",
        "from pyspark.sql.types import StringType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/bin/spark-class: line 71: /opt/homebrew/Cellar/openjdk@17/17.0.16/libexec/openjdk.jdk/Contents/Home/bin/java: No such file or directory\n",
            "/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/bin/spark-class: line 97: CMD: bad array subscript\n",
            "head: illegal line count -- -1\n"
          ]
        },
        {
          "ename": "PySparkRuntimeError",
          "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpark SQL Query Dataframes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m      2\u001b[0m sc \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msparkContext\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/sql/session.py:556\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    554\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    555\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 556\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate(sparkConf)\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    559\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/core/context.py:523\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 523\u001b[0m         SparkContext(conf\u001b[38;5;241m=\u001b[39mconf \u001b[38;5;129;01mor\u001b[39;00m SparkConf())\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/core/context.py:205\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    203\u001b[0m     )\n\u001b[0;32m--> 205\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    208\u001b[0m         master,\n\u001b[1;32m    209\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    220\u001b[0m     )\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/core/context.py:444\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 444\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m launch_gateway(conf)\n\u001b[1;32m    445\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/java_gateway.py:111\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[1;32m    112\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    113\u001b[0m         messageParameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    114\u001b[0m     )\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    117\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
            "\u001b[0;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
          ]
        }
      ],
      "source": [
        "spark = SparkSession.builder.appName(\"Spark SQL Query Dataframes\").getOrCreate()\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": true
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'sc' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mparallelize([Row(server_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m101 Server\u001b[39m\u001b[38;5;124m'\u001b[39m, cpu_utilization\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m85\u001b[39m, session_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m), \\\n\u001b[1;32m      2\u001b[0m                              Row(server_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m101 Server\u001b[39m\u001b[38;5;124m'\u001b[39m, cpu_utilization\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m, session_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m90\u001b[39m),\n\u001b[1;32m      3\u001b[0m                              Row(server_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m102 Server\u001b[39m\u001b[38;5;124m'\u001b[39m, cpu_utilization\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m85\u001b[39m, session_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m),\n\u001b[1;32m      4\u001b[0m                              Row(server_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m103 Server\u001b[39m\u001b[38;5;124m'\u001b[39m, cpu_utilization\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m70\u001b[39m, session_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m),\n\u001b[1;32m      5\u001b[0m                              Row(server_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m104 Server\u001b[39m\u001b[38;5;124m'\u001b[39m, cpu_utilization\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m, session_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m)])\u001b[38;5;241m.\u001b[39mtoDF()\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
          ]
        }
      ],
      "source": [
        "df = spark.createDataFrame([Row(server_name='101 Server', cpu_utilization=85, session_count=80),\n",
        "                             Row(server_name='101 Server', cpu_utilization=80, session_count=90),\n",
        "                             Row(server_name='102 Server', cpu_utilization=85, session_count=40),\n",
        "                             Row(server_name='103 Server', cpu_utilization=70, session_count=80),\n",
        "                             Row(server_name='104 Server', cpu_utilization=60, session_count=80)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+-----------+-------------+\n",
            "|cpu_utilization|server_name|session_count|\n",
            "+---------------+-----------+-------------+\n",
            "|             85| 101 Server|           80|\n",
            "|             80| 101 Server|           90|\n",
            "|             85| 102 Server|           40|\n",
            "|             70| 103 Server|           80|\n",
            "|             60| 104 Server|           80|\n",
            "+---------------+-----------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "df_na = df.withColumn('na_col', lit(None).cast(StringType()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+-----------+-------------+------+\n",
            "|cpu_utilization|server_name|session_count|na_col|\n",
            "+---------------+-----------+-------------+------+\n",
            "|             85| 101 Server|           80|  null|\n",
            "|             80| 101 Server|           90|  null|\n",
            "|             85| 102 Server|           40|  null|\n",
            "|             70| 103 Server|           80|  null|\n",
            "|             60| 104 Server|           80|  null|\n",
            "+---------------+-----------+-------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_na.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+-----------+-------------+------+\n",
            "|cpu_utilization|server_name|session_count|na_col|\n",
            "+---------------+-----------+-------------+------+\n",
            "|             85| 101 Server|           80|     A|\n",
            "|             80| 101 Server|           90|     A|\n",
            "|             85| 102 Server|           40|     A|\n",
            "|             70| 103 Server|           80|     A|\n",
            "|             60| 104 Server|           80|     A|\n",
            "+---------------+-----------+-------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_na.fillna('A').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "df2 = df_na.fillna('A').union(df_na)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+-----------+-------------+------+\n",
            "|cpu_utilization|server_name|session_count|na_col|\n",
            "+---------------+-----------+-------------+------+\n",
            "|             85| 101 Server|           80|     A|\n",
            "|             80| 101 Server|           90|     A|\n",
            "|             85| 102 Server|           40|     A|\n",
            "|             70| 103 Server|           80|     A|\n",
            "|             60| 104 Server|           80|     A|\n",
            "|             85| 101 Server|           80|  null|\n",
            "|             80| 101 Server|           90|  null|\n",
            "|             85| 102 Server|           40|  null|\n",
            "|             70| 103 Server|           80|  null|\n",
            "|             60| 104 Server|           80|  null|\n",
            "+---------------+-----------+-------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+-----------+-------------+------+\n",
            "|cpu_utilization|server_name|session_count|na_col|\n",
            "+---------------+-----------+-------------+------+\n",
            "|             85| 101 Server|           80|     A|\n",
            "|             80| 101 Server|           90|     A|\n",
            "|             85| 102 Server|           40|     A|\n",
            "|             70| 103 Server|           80|     A|\n",
            "|             60| 104 Server|           80|     A|\n",
            "+---------------+-----------+-------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df2.na.drop().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "df2.createOrReplaceTempView(\"na_table\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+-----------+-------------+------+\n",
            "|cpu_utilization|server_name|session_count|na_col|\n",
            "+---------------+-----------+-------------+------+\n",
            "|             85| 101 Server|           80|     A|\n",
            "|             80| 101 Server|           90|     A|\n",
            "|             85| 102 Server|           40|     A|\n",
            "|             70| 103 Server|           80|     A|\n",
            "|             60| 104 Server|           80|     A|\n",
            "|             85| 101 Server|           80|  null|\n",
            "|             80| 101 Server|           90|  null|\n",
            "|             85| 102 Server|           40|  null|\n",
            "|             70| 103 Server|           80|  null|\n",
            "|             60| 104 Server|           80|  null|\n",
            "+---------------+-----------+-------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT * FROM na_table\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+-----------+-------------+------+\n",
            "|cpu_utilization|server_name|session_count|na_col|\n",
            "+---------------+-----------+-------------+------+\n",
            "|             85| 101 Server|           80|  null|\n",
            "|             80| 101 Server|           90|  null|\n",
            "|             85| 102 Server|           40|  null|\n",
            "|             70| 103 Server|           80|  null|\n",
            "|             60| 104 Server|           80|  null|\n",
            "+---------------+-----------+-------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT * FROM na_table WHERE na_col IS NULL\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+-----------+-------------+------+\n",
            "|cpu_utilization|server_name|session_count|na_col|\n",
            "+---------------+-----------+-------------+------+\n",
            "|             85| 101 Server|           80|     A|\n",
            "|             80| 101 Server|           90|     A|\n",
            "|             85| 102 Server|           40|     A|\n",
            "|             70| 103 Server|           80|     A|\n",
            "|             60| 104 Server|           80|     A|\n",
            "+---------------+-----------+-------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT * FROM na_table WHERE na_col IS NOT NULL\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Anaconda Base)",
      "language": "python",
      "name": "anaconda-base"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
